# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import argparse
import glob
import multiprocessing as mp
import os
from natsort import natsorted
import time
import cv2
import tqdm
import slicerio
import numpy as np
import torch
import colorsys

from detectron2.data.detection_utils import read_image
from detectron2.utils.logger import setup_logger

from predictor import VisualizationDemo
from adet.config import get_cfg

from detectron2.data import DatasetCatalog, MetadataCatalog
import random
import json
from detectron2.structures import BoxMode

# constants
WINDOW_NAME = "COCO detections"


def setup_cfg(args):
    # load config from file and command-line arguments
    cfg = get_cfg()
    # Check https://github.com/AugP-creatis/detectron2-Z/blob/main/detectron2/config/defaults.py
    # and https://github.com/AugP-creatis/AdelaiDet-Z/blob/master/adet/config/defaults.py

    cfg.merge_from_file(args.config_file)

    cfg.DATASETS.TEST = ('test',)

    # Other settings you may want to change:

    # STACK
    #cfg.INPUT.STACK_SIZE = 11
    #cfg.INPUT.EXTENSION = "_ch00.tif"
    #cfg.INPUT.SLICE_SEPARATOR = "z"
    #cfg.INPUT.EXCLUDED_PATH_SUBSTRINGS = ("MetaData", "hyperstack", "utilisable", "non_utilisable")

    # INPUT
    #cfg.INPUT.FORMAT = "BGR"   # Model input format (has to be BGR for seamless inference when reading RGB images with opencv)
    #cfg.INPUT.MASK_FORMAT = "polygon"

    cfg.DATALOADER.NUM_WORKERS = 1  #max 2 recommended

    # MODEL
    #cfg.MODEL.USE_AMP = True
    #cfg.MODEL.EARLY_FILTER.ENABLED = True
    #cfg.MODEL.EARLY_FILTER.OPERATOR = "Sobel"
    #cfg.MODEL.MOBILENET = False
    #cfg.MODEL.BACKBONE.INTER_SLICE = True
    #cfg.MODEL.BACKBONE.ANTI_ALIAS = False
    #cfg.MODEL.RESNETS.STRIDE_IN_1X1 = True
    #cfg.MODEL.RESNETS.RES5_DILATION = 1
    #cfg.MODEL.RESNETS.DEFORM_INTERVAL = 1
    #cfg.MODEL.FCOS.INFERENCE_TH_TEST = 0.05
    cfg.MODEL.FCOS.NUM_CLASSES = len(eval(args.classes_dict))  #For FCOS and CondInst

    #cfg.OUTPUT.FILTER_DUPLICATES = False
    
    cfg.merge_from_list(args.opts)
    cfg.freeze()
    return cfg

def get_parser():
    parser = argparse.ArgumentParser(description="Detectron2 Demo")

    parser.add_argument('--classes-dict',type=str,default="{'Intact_Sharp':0}")
    # Classes follows "{'Intact_Sharp':0,'Intact_Blurry':1,'Broken_Sharp':2,'Broken_Blurry':3}"
    # Example : "{'Intact_Sharp':0, 'Broken_Sharp':2}"

    parser.add_argument(
        "--config-file",
        default="BF2i/CondInstZ_BF2i.yaml",
        metavar="FILE",
        help="path to config file",
    )
    parser.add_argument("--input", nargs="+", help="A list of space separated input images")
    parser.add_argument(
        "--opts",
        help="Modify config options using the command-line 'KEY VALUE' pairs",
        default=[],
        nargs=argparse.REMAINDER,
    )
    return parser


def write_3Dslicer_nrrd(voxels, out_filename):
    nb_segments = len(voxels)

    spacing = (0.35277777777777775, 0.35277777777777775, 1)
    origin = (0, 0, 0)

    segmentation = {
        "voxels": voxels,
        "encoding": "gzip",
        "ijkToLPS": [
            [spacing[0], 0., 0., origin[0]],
            [0., spacing[1], 0., origin[1]],
            [0., 0., spacing[2], origin[2]],
            [0., 0., 0., 1. ]
        ],
        "containedRepresentationNames": ["Binary labelmap"],
        "masterRepresentation": "Binary labelmap",
        "referenceImageExtentOffset": [0, 0, 0],
        "segments": [
            {
                "id": f"Segment_{seg_idx}",
                "labelValue": 1,
                "layer": seg_idx,
                "color": colorsys.hls_to_rgb(np.random.uniform(0.5, 1), 0.5, 1),
                "colorAutoGenerated": True,
                "name": f"Segment_{seg_idx}",
                "nameAutoGenerated": True,
                "status": "inprogress",
                "terminology": {
                    "contextName": "Segmentation category and type - 3D Slicer General Anatomy list",
                    "category": ["SCT", "85756007", "Tissue"],
                    "type": ["SCT", "85756007", "Tissue"]
                }
            }
            for seg_idx in range(nb_segments)
        ],
        "conversionParameters": [
            {
                "name": "Decimation factor",
                "value": 0.0,
                "description":  "Desired reduction in the total number of polygons."
                                +" Range: 0.0 (no decimation) to 1.0 (as much simplification as possible)."
                                +" Value of 0.8 typically reduces data set size by 80% without losing too much details."
            },
            {
                "name": "Smoothing factor",
                "value": 0.5,
                "description":  "Smoothing factor."
                                +" Range: 0.0 (no smoothing) to 1.0 (strong smoothing)."
            },
            {
                "name": "Compute surface normals",
                "value": 1,
                "description":  "Compute surface normals."
                                +" 1 (default) = surface normals are computed."
                                +" 0 = surface normals are not computed (slightly faster but produces less smooth surface display)."
            },
            {
                "name": "Joint smoothing",
                "value": 0,
                "description":  "Perform joint smoothing."
            },
            {
                "name": "Reference image geometry",
                "value": "-3.5277777777777775;0;0;0;0;-3.5277777777777775;0;0;0;0;1;0;0;0;0;1;0;399;0;299;0;0;",
                "description":  "Image geometry description string determining the geometry of the labelmap that is created in course of conversion."
                                +" Can be copied from a volume, using the button."
            },
            {
                "name": "Oversampling factor",
                "value": 1,
                "description":  "Determines the oversampling of the reference image geometry."
                                +" If it's a number, then all segments are oversampled with the same value (value of 1 means no oversampling)."
                                +" If it has the value \"A\", then automatic oversampling is calculated."
            },
            {
                "name": "Crop to reference image geometry",
                "value": 0,
                "description":  "Crop the model to the extent of reference geometry."
                                +" 0 (default) = created labelmap will contain the entire model."
                                +" 1 = created labelmap extent will be within reference image extent."
            },
            {
                "name": "Collapse labelmaps",
                "value": 0,                     #different from original nrrd made by Melanie Ribeiro-Lopes
                "description":  "Merge the labelmaps into as few shared labelmaps as possible."
                                +" 1 = created labelmaps will be shared if possible without overwriting each other."
            },
            {
                "name": "Fractional labelmap oversampling factor",
                "value": 1,
                "description":  "Determines the oversampling of the reference image geometry."
                                +" All segments are oversampled with the same value (value of 1 means no oversampling)."
            },
            {
                "name": "Threshold fraction",
                "value": 0.5,
                "description":  "Determines the threshold that the closed surface is created at as a fractional value between 0 and 1."
            }
        ]
    }

    slicerio.write_segmentation(out_filename, segmentation)


if __name__ == "__main__":
    mp.set_start_method("spawn", force=True)
    args = get_parser().parse_args()
    logger = setup_logger()
    logger.info("Arguments: " + str(args))

    cfg = setup_cfg(args)
    classes = eval(args.classes_dict)
    # Set Metadata of the test dataset.
    MetadataCatalog.get('test').set(thing_classes=list(classes.keys()))

    demo = VisualizationDemo(cfg)

    if args.input:
        for input in args.input:
            input = os.path.expanduser(input)
            logger.info("Launch model on {}".format(input))
            stack_size = cfg.INPUT.STACK_SIZE
            sep = cfg.INPUT.SLICE_SEPARATOR
            ext = cfg.INPUT.EXTENSION

            stacks_dict = {}
            if os.path.isdir(input):
                for root, dirs, files in os.walk(input):
                    if all([substring not in root for substring in cfg.INPUT.EXCLUDED_PATH_SUBSTRINGS]):
                        for file in files:
                            if file.endswith(ext):
                                img_path = os.path.join(root, file)
                                img_stack_name = img_path.rsplit(sep, 1)[0]
                                if img_stack_name in stacks_dict:
                                    stacks_dict[img_stack_name].append(img_path)
                                else:
                                    stacks_dict[img_stack_name] = [img_path]
                                if len(stacks_dict[img_stack_name]) == stack_size:
                                    stacks_dict[img_stack_name] = natsorted(
                                        stacks_dict[img_stack_name],
                                        key = lambda p : p.rsplit(sep, 1)[1]
                                    )
            else:   #input is begginning of stack path
                root = os.path.dirname(input)
                for entry in os.listdir(root):
                    path = os.path.join(root, entry)
                    if os.path.isfile(path) and path.endswith(ext) and path.startswith(input):
                        img_stack_name = path.rsplit(sep, 1)[0]
                        if img_stack_name in stacks_dict:
                            stacks_dict[img_stack_name].append(path)
                        else:
                            stacks_dict[img_stack_name] = [path]
                        if len(stacks_dict[img_stack_name]) == stack_size:
                            stacks_dict[img_stack_name] = natsorted(
                                stacks_dict[img_stack_name],
                                key = lambda p : p.rsplit(sep, 1)[1]
                            )

            # Create the list of the stacked dictionaries & verify if it is well constructed
            stacks_paths = list(stacks_dict.values())
            nb_stacks = len(stacks_paths)
            logger.info("Number of stacks: {}".format(nb_stacks))
            cnt_img = 0
            cnt_too_big = 0
            cnt_too_small = 0

            for s in range(nb_stacks):
                cnt_img += len(stacks_paths[s])
                if len(stacks_paths[s]) == stack_size:
                    stack_sorted = True
                    for z in range(stack_size):
                        if int(stacks_paths[s][z].rsplit(sep, 1)[1][:-len(ext)]) != z:
                            stack_sorted = False
                    if not stack_sorted:
                        logger.warning("Stack {} is not sorted ({})".format(s, stacks_paths[s][0].rsplit(sep, 1)[0]))
                elif len(stacks_paths[s]) > stack_size:
                    cnt_too_big += 1
                elif len(stacks_paths[s]) < stack_size:
                    cnt_too_small +=1

            logger.info("There are {} images in total".format(cnt_img))
            assert cnt_too_big == 0, "{} stacks have a bigger size than expected ({})".format(cnt_too_big, stack_size)
            assert cnt_too_small == 0, "{} stacks have a smaller size than expected ({})".format(cnt_too_small, stack_size)
            logger.info("All stacks have {} images".format(stack_size))

            for stack_name, slices_paths in tqdm.tqdm(stacks_dict.items()):
                stack = [None] * stack_size
                for z in range(stack_size):
                    # use PIL, to be consistent with evaluation
                    stack[z] = read_image(slices_paths[z], format=cfg.INPUT.FORMAT)

                start_time = time.time()
                predictions, visualized_output = demo.run_on_stack(stack)

                if cfg.OUTPUT.GATHER_STACK_RESULTS:
                    nb_predictions = len(predictions[0]["instances"])   # same predictions on all images/slices
                else:
                    nb_predictions = 0
                    for z in range(stack_size):
                        nb_predictions += len(predictions[z]["instances"])

                logger.info(
                    "{}: detected {} instances in {:.2f}s".format(
                        stack_name, nb_predictions, time.time() - start_time
                    )
                )

                for z in range(stack_size):
                    out_filename = slices_paths[z][:-len(ext)]
                    if cfg.OUTPUT.IMAGE_FILE:
                        visualized_output[z].save(out_filename + ".png")
                    if cfg.OUTPUT.NRRD_FILE:
                        if predictions[z]["instances"].has("pred_masks"):
                            pred_masks = predictions[z]["instances"].pred_masks.to(torch.device('cpu'), torch.uint8).numpy()
                            pred_masks = np.transpose(pred_masks, (0, 2, 1))
                            nrrd_voxels = np.expand_dims(pred_masks, axis=3)
                            write_3Dslicer_nrrd(nrrd_voxels, out_filename + ".seg.nrrd")
